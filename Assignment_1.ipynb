{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3961453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go through the questions one by one and draft the answers. You can then put these answers into a Jupyter notebook as required.\n",
    "\n",
    "# ### Q1. What is the KNN algorithm?\n",
    "\n",
    "# **Answer:**\n",
    "# K-Nearest Neighbors (KNN) is a simple, non-parametric, and lazy learning algorithm used for classification and regression tasks. It classifies a data point based on how its neighbors are classified. The algorithm stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). For classification, it assigns the majority class among the k-nearest neighbors to the query point. For regression, it averages the values of the k-nearest neighbors.\n",
    "\n",
    "# ### Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "# **Answer:**\n",
    "# Choosing the value of K is crucial for the performance of the KNN algorithm. Common methods include:\n",
    "# - **Cross-validation:** Split the training data into subsets and test different values of K to find the one that minimizes the error.\n",
    "# - **Empirical methods:** Often, an odd value is chosen to avoid ties, and K is set as the square root of the number of data points.\n",
    "# - **Domain knowledge:** Sometimes, domain-specific knowledge helps in choosing the optimal K value.\n",
    "\n",
    "# ### Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "# **Answer:**\n",
    "# - **KNN Classifier:** Used for classification tasks. It assigns the class that is most common among the k-nearest neighbors to the new data point.\n",
    "# - **KNN Regressor:** Used for regression tasks. It predicts the value by averaging the values of the k-nearest neighbors.\n",
    "\n",
    "# ### Q4. How do you measure the performance of KNN?\n",
    "\n",
    "# **Answer:**\n",
    "# Performance can be measured using different metrics depending on the task:\n",
    "# - **Classification:** Accuracy, Precision, Recall, F1 Score, Confusion Matrix.\n",
    "# - **Regression:** Mean Squared Error (MSE), Mean Absolute Error (MAE), R-squared.\n",
    "\n",
    "# ### Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "# **Answer:**\n",
    "# The curse of dimensionality refers to the phenomenon where the feature space becomes increasingly sparse for higher-dimensional data, making it difficult to measure distances and find neighbors. As dimensions increase, the distance between points becomes more uniform, and the algorithm's performance can degrade.\n",
    "\n",
    "# ### Q6. How do you handle missing values in KNN?\n",
    "\n",
    "# **Answer:**\n",
    "# Missing values can be handled in several ways:\n",
    "# - **Imputation:** Fill missing values with the mean, median, or mode of the feature.\n",
    "# - **Remove records:** Remove records with missing values, though this may lead to loss of data.\n",
    "# - **KNN Imputation:** Use KNN to impute missing values by finding the k-nearest neighbors with known values.\n",
    "\n",
    "# ### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "# **Answer:**\n",
    "# - **KNN Classifier:** Better for classification problems where the goal is to assign a label to an input data point. It is effective when the decision boundary is nonlinear.\n",
    "# - **KNN Regressor:** Better for regression problems where the goal is to predict a continuous output value. It works well for datasets with less noise.\n",
    "\n",
    "# ### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "# **Answer:**\n",
    "# **Strengths:**\n",
    "# - Simple to implement and understand.\n",
    "# - No training phase, making it faster for small datasets.\n",
    "# - Flexible in terms of distance metrics.\n",
    "\n",
    "# **Weaknesses:**\n",
    "# - Computationally expensive at prediction time.\n",
    "# - Sensitive to the scale of data and irrelevant features.\n",
    "# - Performance degrades with high-dimensional data (curse of dimensionality).\n",
    "\n",
    "# **Addressing Weaknesses:**\n",
    "# - Use dimensionality reduction techniques (PCA, LDA).\n",
    "# - Normalize/standardize features.\n",
    "# - Use efficient algorithms like KD-trees or Ball-trees for nearest neighbor search.\n",
    "\n",
    "# ### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "# **Answer:**\n",
    "# - **Euclidean Distance:** Measures the straight-line distance between two points in Euclidean space. Formula: \\( \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} \\).\n",
    "# - **Manhattan Distance:** Measures the distance between two points along axes at right angles. Formula: \\( \\sum_{i=1}^{n} |x_i - y_i| \\).\n",
    "\n",
    "# **Euclidean distance** is generally used when the data is continuous, and **Manhattan distance** is used for high-dimensional spaces and when dealing with categorical data.\n",
    "\n",
    "# ### Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "# **Answer:**\n",
    "# Feature scaling is crucial in KNN because it is a distance-based algorithm. Features with larger ranges can dominate the distance calculations, leading to biased results. Scaling methods include normalization (rescaling to [0, 1]) and standardization (rescaling to have a mean of 0 and a standard deviation of 1). Proper scaling ensures that all features contribute equally to the distance computation.\n",
    "\n",
    "# ---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
